{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b34b392",
   "metadata": {},
   "source": [
    "# ATLAS v2 Demo - Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296b21f2",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "01d858d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# imports\n",
    "import ee, os, math, joblib, pickle, json\n",
    "import numpy as np, pandas as pd, tensorflow as tf\n",
    "from datetime import datetime, timezone\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from ast import literal_eval\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from featuretoolkit import src as ftk\n",
    "\n",
    "# authenticating & initializing earth engine\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "ee.Authenticate()\n",
    "ee.Initialize(project=os.getenv('projectkey'))\n",
    "\n",
    "# reading config & setting seed + guaranteeing determinism\n",
    "import yaml\n",
    "with open('config.yaml','r') as f:config_full=yaml.safe_load(f)\n",
    "SEED=config_full.get('seed')\n",
    "RNG=np.random.default_rng(SEED) # initializing global rng\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "tf.keras.utils.set_random_seed(SEED)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2' # suppress tensorflow warnings\n",
    "\n",
    "config=config_full.get('model') # specialize config for only model section - no more feature config required\n",
    "config_path=config.get('path') # path config section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c92277e",
   "metadata": {},
   "source": [
    "## Dataset Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4937b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SF_0: 36 subfolds, total=542477, pos=173315, bg=369162, ratio=0.469\n",
      "SF_1: 36 subfolds, total=542469, pos=152885, bg=389584, ratio=0.392\n",
      "SF_2: 36 subfolds, total=542478, pos=138661, bg=403817, ratio=0.343\n",
      "SF_3: 36 subfolds, total=542472, pos=162061, bg=380411, ratio=0.426\n",
      "SF_4: 36 subfolds, total=542443, pos=119822, bg=422621, ratio=0.284\n",
      "SF_5: 36 subfolds, total=542418, pos=130614, bg=411804, ratio=0.317\n",
      "SF_6: 36 subfolds, total=542533, pos=118381, bg=424152, ratio=0.279\n",
      "SF_7: 36 subfolds, total=542481, pos=77580, bg=464901, ratio=0.167\n",
      "background rescaled by 1/s = 2.315e-08 (log(s) = 17.581)\n",
      "\n",
      "original weights\n",
      "∑w (positives): 7.356e+03\n",
      "∑w (background): 3.178e+11\n",
      "\n",
      "rescaled weights\n",
      "∑w (positives): 7.356e+03\n",
      "∑w (background): 7.356e+03\n"
     ]
    }
   ],
   "source": [
    "dF_pca=pd.read_csv(config_path.get('pca')) # set of all points w/ pca features (add a distinct index column too)\n",
    "\n",
    "ratios=config.get('split_ratio') # train, val, test ratio\n",
    "assert sum(ratios)==1 and len(ratios)==3 # asserting valid split ratios\n",
    "folds=np.sort(dF_pca['fold_id'].unique()) # fold ids\n",
    "splits={}\n",
    "\n",
    "# iterating through each fold to populate fold-wise splits with arrays of dF_pca indices\n",
    "for fold in folds:\n",
    "    idx=dF_pca.index[dF_pca['fold_id']==fold].to_numpy()\n",
    "    # generating unique permutation each iteration while still maintaining reproducibility due to global seed & rng, then splitting indices\n",
    "    perm=RNG.permutation(idx)\n",
    "    n_train=int(len(perm)*ratios[0])\n",
    "    n_val=int(len(perm)*ratios[1])\n",
    "    splits[fold]={\n",
    "        'train_idx':perm[:n_train],\n",
    "        'val_idx':perm[n_train:n_train+n_val],\n",
    "        'test_idx':perm[n_train+n_val:]}\n",
    "\n",
    "splits # split dict organized fold-wise\n",
    "\n",
    "# collecting basic fold stats, merging into superfolds\n",
    "fold_stats=[]\n",
    "for fid,s in splits.items(): # looping through folds\n",
    "    pos=int((dF_pca.loc[s['train_idx'],'key']==1).sum()+(dF_pca.loc[s['val_idx'],'key']==1).sum()+(dF_pca.loc[s['test_idx'],'key']==1).sum())\n",
    "    bg=int((dF_pca.loc[s['train_idx'],'key']==0).sum()+(dF_pca.loc[s['val_idx'],'key']==0).sum()+(dF_pca.loc[s['test_idx'],'key']==0).sum())\n",
    "    fold_stats.append((fid,pos,bg,pos+bg))\n",
    "superfolds={i:{'folds':[],'pos':0,'bg':0,'n':0} for i in range(config.get('folds'))}\n",
    "for fid,pos,bg,n in sorted(fold_stats,key=lambda x:x[3],reverse=True): # largest folds first\n",
    "    target=min(superfolds,key=lambda k:superfolds[k]['n'])\n",
    "    superfolds[target]['folds'].append(fid)\n",
    "    superfolds[target]['pos']+=pos\n",
    "    superfolds[target]['bg']+=bg\n",
    "    superfolds[target]['n']+=n\n",
    "\n",
    "# assembling merged splits together\n",
    "merged_splits={}\n",
    "for sf_id,info in superfolds.items():\n",
    "    merged_splits[sf_id]={'train_idx':[],'val_idx':[],'test_idx':[]}\n",
    "    for fold in info['folds']: # looping through folds in superfold\n",
    "        s=splits[fold]\n",
    "        merged_splits[sf_id]['train_idx'].extend(s['train_idx'])\n",
    "        merged_splits[sf_id]['val_idx'].extend(s['val_idx'])\n",
    "        merged_splits[sf_id]['test_idx'].extend(s['test_idx'])\n",
    "    for k in ('train_idx','val_idx','test_idx'): # converting to numpy arrays\n",
    "        merged_splits[sf_id][k]=np.array(merged_splits[sf_id][k],dtype=int)\n",
    "\n",
    "# summary printout\n",
    "totals,pos_list,neg_list,ratios=[],[],[],[]\n",
    "for sf_id,s in merged_splits.items(): # looping through superfolds\n",
    "    pos=int((dF_pca.loc[s['train_idx'],'key']==1).sum()+(dF_pca.loc[s['val_idx'],'key']==1).sum()+(dF_pca.loc[s['test_idx'],'key']==1).sum())\n",
    "    bg=int((dF_pca.loc[s['train_idx'],'key']==0).sum()+(dF_pca.loc[s['val_idx'],'key']==0).sum()+(dF_pca.loc[s['test_idx'],'key']==0).sum())\n",
    "    total=pos+bg\n",
    "    ratio=pos/bg\n",
    "    totals.append(total)\n",
    "    pos_list.append(pos)\n",
    "    neg_list.append(bg)\n",
    "    ratios.append(ratio)\n",
    "    print(f'SF_{sf_id}: {len(info['folds'])} subfolds, total={total}, pos={pos}, bg={bg}, ratio={ratio:.3f}')\n",
    "\n",
    "# computing total weights for positives & background\n",
    "sum_pos=dF_pca.loc[dF_pca['key']==1,'weight'].sum()\n",
    "sum_bg=dF_pca.loc[dF_pca['key']==0,'weight'].sum()\n",
    "\n",
    "# computing scaling factor (so both totals are comparable) (this is a necessary step because training just doesnt work otherwise)\n",
    "s=sum_bg/max(sum_pos,1e-12)\n",
    "log_s=np.log(s) # natural log of s (saved for later)\n",
    "\n",
    "# create scaled column\n",
    "dF_pca['weight_scaled']=dF_pca['weight'].copy()\n",
    "dF_pca.loc[dF_pca['key']==0,'weight_scaled']/=s\n",
    "\n",
    "# printout\n",
    "print(f'background rescaled by 1/s = {1/s:.3e} (log(s) = {log_s:.3f})')\n",
    "print(f'\\noriginal weights\\n∑w (positives): {dF_pca.loc[dF_pca['key']==1,'weight'].sum():.3e}\\n∑w (background): {dF_pca.loc[dF_pca['key']==0,'weight'].sum():.3e}')\n",
    "print(f'\\nrescaled weights\\n∑w (positives): {dF_pca.loc[dF_pca['key']==1,'weight_scaled'].sum():.3e}\\n∑w (background): {dF_pca.loc[dF_pca['key']==0,'weight_scaled'].sum():.3e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4c5134",
   "metadata": {},
   "source": [
    "## Model Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "16871f01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# initialize hyperparameters\n",
    "H=config.get('hyperparameters')\n",
    "# casting hyperparameters to correct types (strings to float or tuple)\n",
    "for k,v in H.items():\n",
    "    if type(v)==str and 'e' in v:H[k]=float(v)\n",
    "    elif type(v)==str and '(' in v:H[k]=literal_eval(v)\n",
    "\n",
    "# rff+linear head\n",
    "class RandomFourierFeatures(layers.Layer):\n",
    "    # initialization\n",
    "    def __init__(self,input_dim,output_dim,gamma):\n",
    "        super().__init__() # parent init (everything will wrap into a bigger model)\n",
    "        W_init=RNG.normal(scale=np.sqrt(2*gamma),size=(input_dim,output_dim))\n",
    "        b_init=RNG.uniform(0,2*np.pi,size=(output_dim,))\n",
    "        self.W=self.add_weight( # fixed weights\n",
    "            name='W',shape=(input_dim,output_dim),\n",
    "            initializer=tf.constant_initializer(W_init),trainable=False)\n",
    "        self.b=self.add_weight( # fixed biases\n",
    "            name='b',shape=(output_dim,),\n",
    "            initializer=tf.constant_initializer(b_init),trainable=False)\n",
    "    # forward pass\n",
    "    def call(self,x):\n",
    "        z=tf.matmul(x,self.W)+self.b\n",
    "        return tf.sqrt(2/tf.cast(tf.shape(self.W)[1],tf.float32))*tf.cos(z)\n",
    "\n",
    "# rff+linear lgcp model\n",
    "class RFF_LGCP(keras.Model):\n",
    "    # initialization\n",
    "    def __init__(self,input_dim,rff_dim,gamma,l2,mu_clip):\n",
    "        super().__init__() # parent init\n",
    "        self.rff=RandomFourierFeatures(input_dim,rff_dim,gamma) # rff layer\n",
    "        self.mu_clip=mu_clip # clipping range for mu\n",
    "        self.linear=layers.Dense(1,use_bias=True,kernel_regularizer=keras.regularizers.l2(l2),bias_initializer=tf.constant_initializer(-5)) # linear layer w/ l2 regularization against overfit\n",
    "    # forward pass\n",
    "    def call(self,X):\n",
    "        phi=self.rff(X) # this phi has nothing to do with the phi in features.ipynb\n",
    "        mu=self.linear(phi)\n",
    "        return tf.clip_by_value(mu,self.mu_clip[0],self.mu_clip[1])\n",
    "\n",
    "# cox trainer\n",
    "class CoxTrainer(keras.Model):\n",
    "    # intialization\n",
    "    def __init__(self,lgcp_model):\n",
    "        super().__init__() # parent init\n",
    "        self.lgcp_model=lgcp_model\n",
    "    # reduce terms helper (just the math)\n",
    "    def _reduce_terms(self,y,w,mu,add_reg_losses):\n",
    "        pos=tf.cast(tf.equal(y,1),tf.float32)\n",
    "        L_pos=-tf.reduce_sum(w*pos*mu)\n",
    "        L_neg=tf.reduce_sum(w*(1-pos)*tf.exp(mu)) # times lambda\n",
    "        reg=tf.add_n(self.lgcp_model.losses) if (add_reg_losses and self.lgcp_model.losses) else 0\n",
    "        nll_pos_per_event=L_pos/tf.reduce_sum(w*pos)+1e-12\n",
    "        return L_pos+L_neg+reg,nll_pos_per_event # L_pos+L_neg+reg is total loss\n",
    "    \n",
    "    # training step\n",
    "    def train_step(self,data):\n",
    "        # unpacking then reshaping y & w to column vectors\n",
    "        X,y,w=data # unpacking data\n",
    "        y=tf.cast(tf.reshape(y,(-1,1)),tf.float32)\n",
    "        w=tf.cast(tf.reshape(w,(-1,1)),tf.float32)\n",
    "        # gradient tape for automatic differentiation then compute & apply gradients\n",
    "        with tf.GradientTape() as tape: total,nll_pos_evt=self._reduce_terms(y,w,self.lgcp_model(X,training=True),add_reg_losses=True) # gradient tape for automatic differentiation\n",
    "        grads=tape.gradient(total,self.trainable_variables)\n",
    "        if H.get('grad_clip'):grads=[tf.clip_by_norm(g,H['grad_clip']) if g is not None else None for g in grads] # gradient clipping (important)\n",
    "        self.optimizer.apply_gradients(zip(grads,self.trainable_variables))\n",
    "        return {'loss': total,'nll_pos_per_event': nll_pos_evt} # return\n",
    "\n",
    "    # evaluation step\n",
    "    def test_step(self,data):\n",
    "        # unpacking then reshaping y & w to column vectors\n",
    "        X,y,w=data\n",
    "        y=tf.cast(tf.reshape(y,(-1,1)),tf.float32)\n",
    "        w=tf.cast(tf.reshape(w,(-1,1)),tf.float32)\n",
    "        mu=self.lgcp_model(X,training=False) # forward pass\n",
    "        _,nll_pos_evt=self._reduce_terms(y,w,mu,add_reg_losses=False) # compute nll\n",
    "        return {'loss': nll_pos_evt,'nll_pos_per_event': nll_pos_evt} # return\n",
    "\n",
    "# helper for assembling datasets for training loop\n",
    "# basically just a tf.data.Dataset.from_tensor_slices wrapper\n",
    "def make_dataset(df,idx,features,ycol,wcol,batch,shuffle=False):\n",
    "    X=df.loc[idx,features].to_numpy(np.float32)\n",
    "    y=df.loc[idx,ycol].to_numpy(np.float32)\n",
    "    w=df.loc[idx,wcol].to_numpy(np.float32)\n",
    "    ds=tf.data.Dataset.from_tensor_slices((X,y,w))\n",
    "    if shuffle:ds=ds.shuffle(buffer_size=min(200_000,len(idx)),seed=SEED)\n",
    "    return ds.batch(batch).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# training function\n",
    "def train_rff_lgcp(df,split,H):\n",
    "    features=[c for c in df.columns if c.startswith('pca_')] # detecting feature columns\n",
    "    ds_tr=make_dataset(df,split['train_idx'],features,'key','weight_scaled',H['batch_size'],shuffle=True) # training dataset\n",
    "    ds_va=make_dataset(df,split['val_idx'],features,'key','weight_scaled',H['batch_size']) # validation dataset\n",
    "    model=RFF_LGCP(len(features),H['rff_dim'],H['rff_gamma'],H['l2'],H['mu_clip'],SEED) # initializing model\n",
    "    trainer=CoxTrainer(model) # wrapping model in trainer\n",
    "    # using AdamW optimizer rather than Adam (weight decay is important) then define callbacks & fit model then return\n",
    "    opt=keras.optimizers.AdamW(learning_rate=H['learning_rate'],weight_decay=H['weight_decay'])\n",
    "    trainer.compile(optimizer=opt)\n",
    "    callbacks=[\n",
    "        keras.callbacks.ReduceLROnPlateau(monitor='val_nll_pos_per_event',mode='min',factor=0.5,patience=H['plateau_patience'],min_delta=H['min_delta'],min_lr=H['min_lr'],verbose=1),# reduce lr on plateau\n",
    "        keras.callbacks.EarlyStopping(monitor='val_nll_pos_per_event',mode='min',patience=H['patience'],min_delta=H['min_delta'],restore_best_weights=True,verbose=1)] # early stopping based on patience\n",
    "    trainer.fit(ds_tr,validation_data=ds_va,epochs=H['epochs'],callbacks=callbacks,verbose=1)\n",
    "    return trainer,model,features\n",
    "\n",
    "# finally defining the ATLAS class for loading trained models and performing inference\n",
    "class ATLAS:\n",
    "    def __init__(self,dF_pca,merged_splits,H,save_dir=config_path.get('models'),sf_id=None,verbose=True):\n",
    "        # initialization; loads & builds from best superfold model by default unless specified otherwise (from models directory as per config)\n",
    "        self.df=dF_pca\n",
    "        self.splits=merged_splits\n",
    "        self.H=H\n",
    "        self.save_dir=save_dir\n",
    "        # feature columns (ordered pca_1..pca_K)\n",
    "        self.feats=sorted([c for c in self.df.columns if c.startswith('pca_')],key=lambda c:int(c.split('_')[1]))\n",
    "        self.D=len(self.feats)\n",
    "        # choose best superfold unless specified otherwise\n",
    "        if sf_id is None:self.sf_id,self.sf_val_score=self._pick_best_superfold(verbose=verbose)\n",
    "        else:self.sf_id,self.sf_val=sf_id,None\n",
    "\n",
    "        # build & load chosen model for inference\n",
    "        self.model=RFF_LGCP(self.D,H['rff_dim'],H['rff_gamma'],H['l2'],H['mu_clip'])\n",
    "        _=self.model(tf.zeros((1,self.D),dtype=tf.float32))\n",
    "        wpath=os.path.join(self.save_dir,f'rff_lgcp_sf{self.sf_id}.weights.h5')\n",
    "        self.model.load_weights(wpath)\n",
    "        if verbose:print(f'ATLAS: loaded superfold {self.sf_id} @ {wpath}')\n",
    "\n",
    "        # default reference window for window-aware scaling (can be overridden at call time though)\n",
    "        self.ref_area_km2=np.pi*17.0**2 # hardcoding this here however because the dataset was trained on an avg radius of 17 but change at your own risk\n",
    "        self.ref_dt_sec=3600.0 # really wouldn't recommend changing this beyond an hour since the data is only relevant for hour buckets\n",
    "\n",
    "    # pick best sf by val nll/pos\n",
    "    def _pick_best_superfold(self,verbose=True):\n",
    "        scores={}\n",
    "        for sf in sorted(self.splits.keys()): # looping through existing models directory and loading them up\n",
    "            wpath=os.path.join(self.save_dir,f'rff_lgcp_sf{sf}.weights.h5')\n",
    "            if not os.path.exists(wpath):\n",
    "                if verbose:print(f'[skip] no weights for sf {sf} @ {wpath}')\n",
    "                continue\n",
    "            m=RFF_LGCP(self.D,self.H['rff_dim'],self.H['rff_gamma'],self.H['l2'],self.H['mu_clip']) # building models\n",
    "            _=m(tf.zeros((1,self.D),dtype=tf.float32))\n",
    "            m.load_weights(wpath)\n",
    "            val_idx=np.asarray(self.splits[sf]['val_idx'])\n",
    "            total_L_pos=0;total_L_neg=0;total_w_pos=0\n",
    "            bs=self.H.get('batch_size',8192) # establishing batch size\n",
    "            # for each validation batch essentially compile total weights, loss, etc. to eventually find lowest in directory\n",
    "            for start in range(0,len(val_idx),bs):\n",
    "                sl=val_idx[start:start+bs]\n",
    "                Xv=self.df.loc[sl,self.feats].to_numpy(np.float32)\n",
    "                yv=(self.df.loc[sl,'key']==1).to_numpy(np.float32).reshape(-1,1)\n",
    "                wv=self.df.loc[sl,'weight_scaled'].to_numpy(np.float32).reshape(-1,1)\n",
    "                mu=m(Xv,training=False).numpy()\n",
    "                total_L_pos+=(wv*yv*(-mu)).sum() # positive loss\n",
    "                total_L_neg+=(wv*(1-yv)*np.exp(mu)).sum() # neg loss\n",
    "                total_w_pos+=(wv*yv).sum() # positive weight\n",
    "            if total_w_pos<=0:\n",
    "                if verbose:print(f'ATLAS: sf {sf} has zero positive weight in val; skipping') # probably means wrong directory\n",
    "                continue\n",
    "            score=(total_L_pos+total_L_neg+tf.add_n(m.losses).numpy())/total_w_pos # the tf.add... is the regularization\n",
    "            scores[sf]=score\n",
    "            if verbose:print(f'[sf {sf}] val_nll_per_pos={score:.4f}')\n",
    "        if not scores:raise RuntimeError('no scored models found in directory')\n",
    "        best_sf=min(scores,key=scores.get) # found best superfold, return it + scores\n",
    "        return best_sf,scores[best_sf]\n",
    "\n",
    "    # fit logistic calibrator: μ to R_ref in [0,1] for given reference window\n",
    "    # this is just a simple logistic regression that takes the output mu value and converts it to an appropriate and easier-to-interpret risk value\n",
    "    def fit_logit_calibrator(self,save_path=None,verbose=True):\n",
    "        vidx=np.asarray(self.splits[self.sf_id]['val_idx']) # loading validation ids (all) before constructing batches\n",
    "        Xv=self.df.loc[vidx,self.feats].to_numpy(np.float32)\n",
    "        yv=self.df.loc[vidx,'key'].to_numpy(np.int32)\n",
    "        wv=self.df.loc[vidx,'weight_scaled'].to_numpy(np.float64)\n",
    "        mu_v=self.model.predict(Xv,batch_size=self.H.get('batch_size',8192),verbose=0).ravel()\n",
    "        lr=LogisticRegression(solver='lbfgs',max_iter=500)\n",
    "        lr.fit(mu_v.reshape(-1,1),yv,sample_weight=wv)\n",
    "        self._logit=lr\n",
    "        if verbose:\n",
    "            a=float(lr.coef_[0,0]);b=float(lr.intercept_[0])\n",
    "            print(f'[logit] fitted: R=σ({a:.4f}·μ+{b:.4f})')\n",
    "        if save_path:joblib.dump(lr,save_path)\n",
    "        return self._logit\n",
    "    \n",
    "    # load in logit calibrator\n",
    "    def load_logit_calibrator(self,load_path,verbose=True):\n",
    "        self._logit=joblib.load(load_path) # load from path -- self explanatory\n",
    "        if verbose:\n",
    "            a=float(self._logit.coef_[0,0]);b=float(self._logit.intercept_[0])\n",
    "            print(f'[logit] loaded: R=σ({a:.4f}·μ+{b:.4f})')\n",
    "        return self._logit\n",
    "\n",
    "    # μ→risk (reference window). im gonna just old name (prob) as alias for safety and because the code wont work otherwise but this is referring to the risk score\n",
    "    def risk_from_mu_logit(self,mu):\n",
    "        if not hasattr(self,'_logit'):raise RuntimeError('no logistic calibrator; call fit_logit_calibrator() or load_logit_calibrator()') # hint hint\n",
    "        mu=np.asarray(mu,dtype=np.float64).reshape(-1,1)\n",
    "        return self._logit.predict_proba(mu)[:,1]\n",
    "    prob_from_mu_logit=risk_from_mu_logit # alias (back-compat)\n",
    "\n",
    "    # main inferential function to predict >=1 landslide event probability in a window (given uniform subsamples over window W=A·Δt)\n",
    "    # note that window area is in km^2 and dt is in seconds\n",
    "    # this produces a single inference and you can plug in the output from the phi function directly into this pretty much\n",
    "    def risk_score(self,df_pca_subsamples,window_area,dt,batch_size=None):\n",
    "        # columnwise alignment and then guarantee matching dimensionality\n",
    "        cols=sorted([c for c in df_pca_subsamples.columns if c.startswith('pca_')],key=lambda c:int(c.split('_')[1]))\n",
    "        assert len(cols)==self.D,f'PCA dimensionality mismatch: model expects D={self.D},got {len(cols)}'\n",
    "        X=df_pca_subsamples[cols].to_numpy(np.float32)\n",
    "        bs=int(batch_size or self.H.get('batch_size',8192))\n",
    "\n",
    "        mu=self.model.predict(X,batch_size=bs,verbose=0).ravel().astype(np.float64)\n",
    "        r_i=self.risk_from_mu_logit(mu) # per-row risk for reference window\n",
    "        r_i=np.clip(r_i,0.0,1.0)\n",
    "\n",
    "        # simple, stable aggregator for ref-window risk over K rows\n",
    "        R_ref=float(np.clip(np.mean(r_i),0.0,1.0))\n",
    "\n",
    "        # ref→target window scaling via Poisson mapping\n",
    "        A_ref=getattr(self,'ref_area_km2',np.pi*17.0**2)\n",
    "        T_ref=getattr(self,'ref_dt_sec',3600.0)\n",
    "        Lambda_ref=-np.log(max(1.0-R_ref,1e-12)) # just ensuring the R_ref isnt 0 which it shouldnt be unless you change it\n",
    "        scale=(window_area*dt)/(A_ref*T_ref)\n",
    "        Lambda=float(np.clip(Lambda_ref*scale,0.0,1e12))\n",
    "        R=float(np.clip(1.0-np.exp(-Lambda),0.0,1.0))\n",
    "        return Lambda,R\n",
    "\n",
    "    # raw μ for a batch of rows\n",
    "    def mu(self,df_pca,batch_size=None):\n",
    "        # columnwise alignment and then guarantee matching dimensionality\n",
    "        cols=sorted([c for c in df_pca.columns if c.startswith('pca_')],key=lambda c:int(c.split('_')[1]))\n",
    "        assert len(cols)==self.D,f'ATLAS: PCA dimensionality mismatch: model expects D={self.D},got {len(cols)}'\n",
    "        X=df_pca[cols].to_numpy(np.float32)\n",
    "        bs=int(batch_size or self.H.get('batch_size',8192))\n",
    "        return self.model.predict(X,batch_size=bs,verbose=0).ravel()\n",
    "\n",
    "    # best μ threshold by weighted F1\n",
    "    # just loop through some values we know mu will be around pretty much and find the best one\n",
    "    # change n_grid as needed for more or less precision\n",
    "    def best_mu_threshold(self,mu,y,w,n_grid=200):\n",
    "        mu=np.asarray(mu,dtype=np.float64)\n",
    "        y=np.asarray(y,dtype=np.int32)\n",
    "        w=np.asarray(w,dtype=np.float64)\n",
    "        grid=np.quantile(mu,np.linspace(0,1,n_grid))\n",
    "        best=(-1.0,float(np.median(mu)))\n",
    "        for t in grid: # essentially looping through a linear space of mu threshold values to approximate the optimal point\n",
    "            yp=(mu>=t).astype(np.int32)\n",
    "            TP=float(w[(yp==1)&(y==1)].sum())\n",
    "            FP=float(w[(yp==1)&(y==0)].sum())\n",
    "            FN=float(w[(yp==0)&(y==1)].sum())\n",
    "            prec=TP/(TP+FP+1e-12);rec=TP/(TP+FN+1e-12) # calculating precision in order to get f1 score, then just scoring\n",
    "            f1=2*prec*rec/(prec+rec+1e-12)\n",
    "            if f1>best[0]:best=(f1,float(t))\n",
    "        return best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf47e7c",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74e99df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get feature-related & sampling-related config dicts\n",
    "config_features=config_full.get('features').get('features')\n",
    "config_sampling=config_full.get('features').get('sampling')\n",
    "unmask_val=config_sampling.get('unmask_val')\n",
    "SCALE_METRIC=ee.Number(config_sampling.get('scale'))\n",
    "\n",
    "# import all necessary images/imagecollections used in the samploing process\n",
    "nasa_dem=ee.Image('USGS/SRTMGL1_003').resample('bilinear')\n",
    "openlandmap_sand=ee.Image('OpenLandMap/SOL/SOL_SAND-WFRACTION_USDA-3A1A1A_M/v02').resample('bilinear')\n",
    "openlandmap_clay=ee.Image('OpenLandMap/SOL/SOL_CLAY-WFRACTION_USDA-3A1A1A_M/v02').resample('bilinear')\n",
    "esa_worldcover=ee.ImageCollection('ESA/WorldCover/v200').first().rename('land_class')\n",
    "esa_worldcover=esa_worldcover.reproject(crs=esa_worldcover.projection(),scale=10) # reprojecting worldcover to get nearest-pixel resampling\n",
    "nasa_gpm=ee.ImageCollection('NASA/GPM_L3/IMERG_V07').select('precipitation')\n",
    "copernicus_era5=ee.ImageCollection('ECMWF/ERA5_LAND/HOURLY').select([*[f'volumetric_soil_water_layer_{level}' for level in config_features.get('vsw_levels')],'potential_evaporation_hourly','surface_runoff','sub_surface_runoff','snow_cover','snow_depth_water_equivalent']) # selecting only relevant bands\n",
    "nasa_modis_terraveg=ee.ImageCollection('MODIS/061/MOD13Q1')\n",
    "upa=ee.Image('MERIT/Hydro/v1_0_1').resample('bilinear').select('upa') # upstream drainage area to calculate specific catchment area for terrain wetness\n",
    "vwc_0033kPa=ee.Image('ISRIC/SoilGrids250m/v2_0/wv0033').resample('bilinear').unmask(-99) # volumetric water content at 33kPa suction level\n",
    "vwc_1500kPa=ee.Image('ISRIC/SoilGrids250m/v2_0/wv1500').resample('bilinear').unmask(-99) # volumetric water content at 1.5mPa suction level\n",
    "\n",
    "# slightly adjusted phi function for ease of passing predictions into ATLAS (for demonstration), although this is a mostly identical copy of the original phi function and is not intended for direct use outside of ATLAS or for real deployment/sampling purposes\n",
    "# the automatic assignment of a K value based on spacetime volume is removed here, giving the user freedom to specify K (subsampling accuracy) directly.\n",
    "# the adjustments take into account some of the post-processing steps performed in the features.ipynb notebook after running phi originally like constructing a snow mask and running principal component analysis on the features\n",
    "# this function will not work properly with an altered config.yaml file unless the same post-processing steps are also applied to the original feature dataframe from features.ipynb which is a process that takes forever; use at your own risk\n",
    "def phi(pointer:ee.geometry.Geometry,timestamp:ee.Number,radius:ee.Number,K:ee.Number=128,ignore:set={'precip_hits_72h'},seed:ee.Number=ee.Number(SEED)):\n",
    "\n",
    "    # helper function to convert pointer & radius (km) to a roughly circular ee geometry object for subsampling (returns pointer if radius is 0)\n",
    "    def uncertainty_region(pointer:ee.geometry.Geometry=pointer,radius:ee.Number=radius): \n",
    "        return ee.Algorithms.If(radius.eq(0),pointer,pointer.buffer(radius.multiply(1000)))\n",
    "    region=uncertainty_region()\n",
    "\n",
    "    # helper function to randomly & uniformly samples the spacetime area specified by parameters K times, outputs only specific timestamps & coordinate locations\n",
    "    def generate_samples(region:ee.geometry.Geometry=region,timestamp:ee.Number=timestamp,K:ee.Number=K):\n",
    "        start=ee.Date(timestamp)\n",
    "        pts=ee.FeatureCollection.randomPoints(region=region,points=K,seed=seed).randomColumn('u',seed=seed.add(1))\n",
    "        def _add_time(feature):\n",
    "            ts=start.millis().add(ee.Number(3600000).multiply(feature.get('u')))\n",
    "            return feature.set({'ts':ts,'date':ee.Date(ts)})\n",
    "        return pts.map(_add_time)\n",
    "    # initialize subsamples, calculate which buckets are occupied by aggreating u values into a list & converting into hour intervals then finding unique values\n",
    "    subsamples=generate_samples().map(lambda f:f.set('bucket',ee.Number(f.get('u')).multiply(24).floor().int()))\n",
    "    occupied_buckets=subsamples.aggregate_array('bucket').distinct().sort()\n",
    "    subsamples=subsamples.map(lambda f:f.set('bkt_idx',occupied_buckets.indexOf(ee.Number(f.get('bucket'))).int()))\n",
    "    timestamps=occupied_buckets.map(lambda b:ee.Number(timestamp).add(ee.Number(b).multiply(3600000)).add(900000)) # calculate exact quarter-way timestamp for each occupied bucket as ee.List of millis (serves as a kind of buffer)\n",
    "\n",
    "    # helper to flatten & convert list of images to singular multibanded image\n",
    "    def cat_list(imgs:ee.List):\n",
    "        imgs=ee.List(imgs).flatten()\n",
    "        def _cat(img,acc):return ee.Image(acc).addBands(ee.Image(img))\n",
    "        return ee.Image(imgs.slice(1).iterate(_cat,ee.Image(imgs.get(0))))\n",
    "\n",
    "    # array collapse mechanism (dynamic events are temporally flattened into 3d images that, when point-sampled, return an array representing different values over varying timesteps)\n",
    "    # this function 'collapses' array images into scalar images depending on which time step is relevant for any given sample, ignoring all others\n",
    "    def array_collapse(fc:ee.FeatureCollection,band_names:ee.List):\n",
    "        def _collapse(f):\n",
    "            bkt=ee.Number(f.get('bkt_idx'))\n",
    "            def _set_scalar(pname,acc):\n",
    "                acc=ee.Feature(acc)\n",
    "                pname=ee.String(pname)\n",
    "                val=ee.Algorithms.If(acc.propertyNames().contains(pname),ee.Number(ee.Array(acc.get(pname)).get([bkt])),ee.Number(unmask_val))\n",
    "                return acc.set(pname.replace('_arr$',''),val)\n",
    "            return ee.Feature(band_names.iterate(_set_scalar,f))\n",
    "        return fc.map(_collapse)\n",
    "    \n",
    "    # helper function to fill coastal holes for era5 land images (some coastal areas return null arrays due to masking errors around coastal regions)\n",
    "    def fill_coastal_ic(ic:ee.ImageCollection,radius_px:int=3):\n",
    "        def _fill_coastal(img,radius_px:int=3):\n",
    "            img=ee.Image(img)\n",
    "            mean=img.focal_mean(radius=radius_px,units='pixels')\n",
    "            # fill masked w/ neighborhood mean, then any remaining w/ unmasking value\n",
    "            return img.unmask(mean).unmask(unmask_val)\n",
    "        return ee.ImageCollection(ic).map(lambda i:_fill_coastal(i,radius_px))\n",
    "    \n",
    "    # child phi function that specializes in dynamic imagecollections (in this case those with a <1d temporal cadence)\n",
    "    # dynamic images are structured as array stacks which are collapsed later on; all specified windows are defaults -- these numbers are customizable through the config.yaml file\n",
    "    def phi_dynamic():\n",
    "\n",
    "        # helper function to convert ic hourly cadence to daily aggregate\n",
    "        def hourly_to_daily_mean(ic:ee.ImageCollection):\n",
    "            ic=fill_coastal_ic(ic,3)\n",
    "            # get start & end dates of ic\n",
    "            start_date=ee.Date(ic.first().get('system:time_start'))\n",
    "            end_date=ee.Date(ic.sort('system:time_start',False).first().get('system:time_start'))\n",
    "            # helper function to get mean for a single day\n",
    "            def _day_mean(offset):\n",
    "                current_day=start_date.advance(offset,'day')\n",
    "                next_day=current_day.advance(1,'day')\n",
    "                # filter collection for current day, take mean for all images in filtered ic, set time start property for new resultant img & return\n",
    "                mean_img=ic.filterDate(current_day,next_day).mean()\n",
    "                return mean_img.set('system:time_start',current_day.millis())\n",
    "            # derive number of days, convert list of images into ic after mapping over generated list of daily timestamps\n",
    "            num_days=end_date.difference(start_date,'day').round()\n",
    "            return ee.ImageCollection.fromImages(ee.List.sequence(0,num_days.subtract(1)).map(_day_mean))\n",
    "        \n",
    "        # array stacking helper function that stacks an imagecollection into one (technically 3-dimensional) image with pixels that return array values \n",
    "        def array_stack(ic:ee.ImageCollection,hrs:ee.Number,reducer:ee.Reducer,name:ee.String):\n",
    "            ic=fill_coastal_ic(ic,3).map(lambda i:ee.Image(i).resample('bilinear'))\n",
    "            return ee.ImageCollection.fromImages(\n",
    "                timestamps.map(lambda ts:ic.filterDate(ee.Number(ts).subtract(ee.Number(hrs).multiply(3600000)),ee.Number(ts)).reduce(reducer))\n",
    "            ).toBands().resample('bilinear').toArray().rename([name.cat('_arr')])\n",
    "        \n",
    "        # derive hourly precipitation sums over past 1, 6, 24, 72, 240, 720 hrs\n",
    "        precip_sum_hrs=ee.List(config_features.get('dynamic')['precip_sum_h'])\n",
    "        precip_sum_img=cat_list(precip_sum_hrs.map(\n",
    "            lambda h:array_stack(\n",
    "                nasa_gpm.select('precipitation'),h,ee.Reducer.sum(),\n",
    "                ee.String('precip_mm_').cat(ee.Number(h).format()).cat('h_sum'))))\n",
    "                \n",
    "        # derive hourly precipitation maxes over past 6, 12, 24 hrs\n",
    "        precip_max_hrs=ee.List(config_features.get('dynamic')['precip_max_h'])\n",
    "        precip_max_img=cat_list(precip_max_hrs.map(\n",
    "            lambda h:array_stack(\n",
    "                nasa_gpm.select('precipitation'),h,ee.Reducer.max(),\n",
    "                ee.String('precip_mm_').cat(ee.Number(h).format()).cat('h_max'))))\n",
    "\n",
    "        # derive the number of half hours for 72 hours before timestamp where precipitation exceeded 5mm (hits)\n",
    "        precip_hits_hrs=ee.List(config_features.get('dynamic')['precip_hits_h'])\n",
    "        def precip_hits_stack(hrs:int):\n",
    "            hrs=ee.Number(hrs)\n",
    "            return ee.ImageCollection.fromImages(\n",
    "                timestamps.map(lambda ts:nasa_gpm.filterDate(ee.Number(ts).subtract(hrs.multiply(3600000)),ee.Number(ts)).map(lambda img:img.multiply(0.5).gt(10).rename('hit').int().updateMask(img.mask())).sum())\n",
    "            ).toBands().toArray().rename([ee.String('precip_hits_').cat(hrs.format()).cat('h_arr')])\n",
    "        precip_hits_img=cat_list(precip_hits_hrs.map(precip_hits_stack))\n",
    "\n",
    "        # derive soil runoff (surface & subsurface) over past 1, 6, 24, 72 hrs \n",
    "        runoff_hrs=ee.List(config_features.get('dynamic')['runoff_h'])\n",
    "        surface_runoff_sum_img=cat_list(runoff_hrs.map(\n",
    "            lambda h:array_stack(\n",
    "                copernicus_era5.select('surface_runoff'),h,ee.Reducer.sum(),\n",
    "                ee.String('surface_runoff_').cat(ee.Number(h).format()).cat('h_sum'))))\n",
    "        subsurface_runoff_sum_img=cat_list(runoff_hrs.map(\n",
    "            lambda h:array_stack(\n",
    "                copernicus_era5.select('sub_surface_runoff'),h,ee.Reducer.sum(),\n",
    "                ee.String('subsurface_runoff_').cat(ee.Number(h).format()).cat('h_sum'))))\n",
    "        \n",
    "        # derive mean hourly volumetric soil water content (soil moisture) over past 24, 72 hours @ depth levels 1 (0-7cm), 4 (100-289cm); (4 bands total)\n",
    "        sm_hrs=ee.List(config_features.get('dynamic')['soil_moist_mean_h'])\n",
    "        sm_levels=ee.List(config_features.get('vsw_levels'))\n",
    "        sm_labels=ee.List(config_features.get('vsw_labels'))\n",
    "\n",
    "        # stack images by depth\n",
    "        def _vsw_mean_depthwise(level:str,label:str):\n",
    "            level=ee.Number(level);label=ee.String(label)\n",
    "            depth_ic=copernicus_era5.select(ee.String('volumetric_soil_water_layer_').cat(level.format()))\n",
    "            return ee.Image.cat(sm_hrs.map(lambda h:array_stack(\n",
    "                depth_ic,h,ee.Reducer.mean(),\n",
    "                ee.String('soil_moist_').cat(label).cat('_').cat(ee.Number(h).format()).cat('h_mean'))))\n",
    "        sm_depths_img=cat_list(sm_levels.zip(sm_labels).map(lambda pair:_vsw_mean_depthwise(ee.List(pair).get(0),ee.List(pair).get(1))))\n",
    "        \n",
    "        # derive soil moisture anomaly (z-score) and trend (slope of linear fit) over past 7, 90 days @ depth levels 1 (0-7cm), 4 (100-289cm)\n",
    "        sm_levels=ee.List(config_features.get('vsw_levels'))\n",
    "        sm_labels=ee.List(config_features.get('vsw_labels'))\n",
    "        sm_anom_days=ee.List(config_features.get('dynamic')['soil_moist_anom_d'])\n",
    "        sm_trend_days=ee.List(config_features.get('dynamic')['soil_moist_trend_d'])\n",
    "\n",
    "        def _vsw_daily_for_level(level):\n",
    "            level=ee.Number(level)\n",
    "            band=ee.String('volumetric_soil_water_layer_').cat(level.format())\n",
    "            hourly=copernicus_era5.select(band).filterDate(ee.Date(ee.Number(timestamps.reduce(ee.Reducer.min()))).advance(ee.Number(sm_anom_days.reduce(ee.Reducer.max())).max(ee.Number(sm_trend_days.reduce(ee.Reducer.max()))).multiply(-1),'day'),ee.Date(ee.Number(timestamps.reduce(ee.Reducer.max()))))\n",
    "            # daily mean with band name 'sm'\n",
    "            return hourly_to_daily_mean(hourly).select([0],['sm'])\n",
    "\n",
    "        # soil anomaly (z score)\n",
    "        def _vsw_anom_depthwise(level,label):\n",
    "            level=ee.Number(level)\n",
    "            label=ee.String(label)\n",
    "            vsw_daily=_vsw_daily_for_level(level)\n",
    "            def _stack_for_days(days):\n",
    "                days=ee.Number(days)\n",
    "                def _z_for_ts(ts):\n",
    "                    ts=ee.Number(ts)\n",
    "                    stats=vsw_daily.filterDate(ee.Date(ts.subtract(days.multiply(86400000))),ee.Date(ts)).reduce(ee.Reducer.mean().combine(ee.Reducer.stdDev(),'_',True))\n",
    "                    today=vsw_daily.filterDate(ee.Date(ts).advance(-1,'day'),ee.Date(ts)).first()\n",
    "                    # handle potential empty today window defensively\n",
    "                    today=ee.Image(ee.Algorithms.If(today,today,ee.Image.constant(0).rename('sm')))\n",
    "                    return ee.Image(today).subtract(stats.select('sm_mean')).divide(stats.select('sm__stdDev').max(1e-6))\n",
    "                stack=ee.ImageCollection.fromImages(timestamps.map(_z_for_ts)).toBands().toArray()\n",
    "                return stack.rename([ee.String('soil_moist_anom_').cat(days.format()).cat('d_').cat(label).cat('_arr')])\n",
    "            return ee.Image.cat(sm_anom_days.map(_stack_for_days))\n",
    "        sm_anom_img=cat_list(sm_levels.zip(sm_labels).map(lambda pair:_vsw_anom_depthwise(ee.List(pair).get(0),ee.List(pair).get(1))))\n",
    "\n",
    "        # soil trend (linear slope)\n",
    "        def _vsw_trend_depthwise(level,label):\n",
    "            level=ee.Number(level)\n",
    "            label=ee.String(label)\n",
    "            def _stack_for_days(days):\n",
    "                days=ee.Number(days)\n",
    "                def _slope_for_ts(ts):\n",
    "                    ts=ee.Number(ts)\n",
    "                    start=ts.subtract(days.multiply(86400000))\n",
    "                    def _add_t(img):\n",
    "                        img=ee.Image(img).rename('sm')\n",
    "                        return img.addBands(ee.Image.constant(ee.Date(img.get('system:time_start')).difference(ee.Date(start),'day')).rename('t').toFloat()).select(['t','sm'])\n",
    "                    seq=_vsw_daily_for_level(level).filterDate(ee.Date(start),ee.Date(ts)).map(_add_t)\n",
    "                    return ee.ImageCollection(seq).reduce(ee.Reducer.linearFit()).select('scale')\n",
    "                stack=ee.ImageCollection.fromImages(timestamps.map(_slope_for_ts)).toBands().toArray()\n",
    "                return stack.rename([ee.String('soil_moist_trend_').cat(days.format()).cat('d_').cat(label).cat('_arr')])\n",
    "            return ee.Image.cat(sm_trend_days.map(_stack_for_days))\n",
    "        sm_trend_img=cat_list(sm_levels.zip(sm_labels).map(lambda pair:_vsw_trend_depthwise(ee.List(pair).get(0),ee.List(pair).get(1))))\n",
    "\n",
    "        # derive sum of potential evaporation/evapotranspiration over antecedent 7d window\n",
    "        pev_days=ee.Number(config_features.get('dynamic')['pev_sum_d'])\n",
    "        pev_hrs=pev_days.multiply(24)\n",
    "        pev_sum_arr=array_stack(\n",
    "            copernicus_era5.select('potential_evaporation_hourly'),pev_hrs,ee.Reducer.sum(),\n",
    "            ee.String('pev_sum_').cat(pev_days.format()).cat('d'))\n",
    "        \n",
    "        # derive moisture deficit calculated by the ReLU of potential evaporation minus precipitation over antecedent 7d window\n",
    "        precip_sum_pev=array_stack( # calculating 7 day precipitation sum (only for moisture deficit calculation; not returned)\n",
    "            nasa_gpm.select('precipitation'),pev_hrs,ee.Reducer.sum(),\n",
    "            ee.String('precip_mm_').cat(pev_hrs.format()).cat('h_sum_for_pev'))\n",
    "        moisture_deficit_img=pev_sum_arr.subtract(precip_sum_pev).rename([ee.String('moisture_deficit_').cat(pev_days.format()).cat('d_arr')])\n",
    "        \n",
    "        # derive percentage snow cover of the given pixel (of the era5 image) and snow depth in terms of water equivalent\n",
    "        snow_hrs=ee.List(config_features.get('dynamic')['snow_h'])\n",
    "        snow_cover_img=cat_list(snow_hrs.map(\n",
    "            lambda h:array_stack(\n",
    "                copernicus_era5.select('snow_cover'),h,ee.Reducer.sum(),\n",
    "                ee.String('snow_cover_').cat(ee.Number(h).format()).cat('h_sum'))))\n",
    "        snow_depth_img=cat_list(snow_hrs.map(\n",
    "            lambda h:array_stack(\n",
    "                copernicus_era5.select('snow_depth_water_equivalent'),h,ee.Reducer.sum(),\n",
    "                ee.String('snow_depth_').cat(ee.Number(h).format()).cat('h_sum'))))\n",
    "\n",
    "        # day of year trig encoding before passing on to static so that model is somewhat temporally aware\n",
    "        tau=ee.Number(2*math.pi)\n",
    "        def _angle_for_ts(ts):\n",
    "            d=ee.Date(ts)\n",
    "            doy=d.difference(d.getRange('year').start(),'day').add(1)\n",
    "            return doy.divide(365.25).multiply(tau)\n",
    "        doy_sin_arr=ee.ImageCollection.fromImages(\n",
    "            timestamps.map(lambda ts:ee.Image.constant(_angle_for_ts(ts).sin()))\n",
    "        ).toBands().toArray().rename(['doy_sin_arr'])\n",
    "        doy_cos_arr=ee.ImageCollection.fromImages(\n",
    "            timestamps.map(lambda ts:ee.Image.constant(_angle_for_ts(ts).cos()))\n",
    "        ).toBands().toArray().rename(['doy_cos_arr'])\n",
    "        \n",
    "        # concatenate & return dynamic img stack\n",
    "        dynamic_array_img=ee.Image.cat([\n",
    "            precip_sum_img,\n",
    "            precip_max_img,\n",
    "            precip_hits_img,\n",
    "            surface_runoff_sum_img,\n",
    "            subsurface_runoff_sum_img,\n",
    "            sm_depths_img,\n",
    "            sm_anom_img,\n",
    "            sm_trend_img,\n",
    "            pev_sum_arr,\n",
    "            moisture_deficit_img,\n",
    "            snow_cover_img,\n",
    "            snow_depth_img,\n",
    "            doy_sin_arr,\n",
    "            doy_cos_arr])\n",
    "        return dynamic_array_img\n",
    "    \n",
    "    def phi_static(timestamp:ee.Number=timestamp):\n",
    "\n",
    "        # terrain slope in degrees\n",
    "        terrain_slope=ee.Terrain.slope(nasa_dem).rename('slope_deg')\n",
    "\n",
    "        # terrain aspect sin & cos\n",
    "        aspect_deg=ee.Terrain.aspect(nasa_dem)\n",
    "\n",
    "        aspect_sin=aspect_deg.multiply(math.pi/180).sin().rename('aspect_sin')\n",
    "        aspect_cos=aspect_deg.multiply(math.pi/180).cos().rename('aspect_cos')\n",
    "\n",
    "        # terrain ruggedness\n",
    "        kernel=ee.Kernel.square(radius=1,units='pixels',normalize=False) # kernel for ruggedness\n",
    "        tri=nasa_dem.neighborhoodToBands(kernel).subtract(nasa_dem).pow(2).reduce(ee.Reducer.sum()).sqrt().rename('tri') # tri (terrain ruggedness index)\n",
    "\n",
    "        # topographic wedness index\n",
    "        proj=upa.projection()\n",
    "        sca=upa.multiply(1e6).divide(ee.Image.pixelArea().reproject(proj).sqrt()).max(1) # specific catchment area\n",
    "        tan_slope=ee.Terrain.slope(nasa_dem.reproject(proj)).multiply(math.pi/180).tan().max(1e-6) # tangent of slope\n",
    "        twi=sca.divide(tan_slope).log().rename('twi') # twi (topographic wetness index) derived from sca (specific catchment area) & tan of slope in rad \n",
    "\n",
    "        # laplacian curvature (signed second derivative of elevation) derived via built-in laplacian kernel\n",
    "        dem_window=nasa_dem.clip(ee.Geometry(region).buffer(SCALE_METRIC.multiply(8)))\n",
    "        base=ee.Image(1).rename('base').clip(ee.Geometry(region).buffer(SCALE_METRIC.multiply(8)))\n",
    "        curvature=dem_window.convolve(ee.Kernel.laplacian4()).divide(ee.Image.pixelArea()).rename('curvature_laplace').updateMask(base.mask())\n",
    "\n",
    "        # sand and clay content (4 bands each for each depth)\n",
    "        sc_depths=ee.List(config_features.get('static')['sc_depths'])\n",
    "        sand_imgs=sc_depths.map(lambda d:openlandmap_sand.select(ee.String(d)).rename([ee.String('sand_content_').cat(ee.String(d))]))\n",
    "        sand_pc=cat_list(sand_imgs)\n",
    "        clay_imgs=sc_depths.map(lambda d:openlandmap_clay.select(ee.String(d)).rename([ee.String('clay_content_').cat(ee.String(d))]))\n",
    "        clay_pc=cat_list(clay_imgs)\n",
    "        \n",
    "        # static volumetric soil water content at 0-5cm and 5-15cm depth at 10kPa, 33kPa, & 1500kPa suctions (6 bands)\n",
    "        depths=ee.List(config_features.get('static')['vwc_depths'])\n",
    "        quant=ee.String(config_features.get('static')['vwc_quantile'])\n",
    "        def _vwc_for(img:ee.Image,suction:ee.String):return ee.Image(img).select(depths.map(lambda d:ee.String('val_').cat(ee.String(d)).cat('_').cat(quant)),depths.map(lambda d: ee.String('vwc_kPa').cat(suction).cat('_').cat(ee.String(d))))\n",
    "        vwc=ee.Image.cat([\n",
    "            _vwc_for(vwc_0033kPa,ee.String('0033')),\n",
    "            _vwc_for(vwc_1500kPa,ee.String('1500'))])\n",
    "        \n",
    "        # derive z score of ndvi over antecedent 96d window - technically variable over time but can still be treated as static because of >1d cadence\n",
    "        ndvi_days=config_features.get('static')['ndvi_anom_d']\n",
    "        def _ndvi_anom(timestamp:ee.Number,days:int):\n",
    "            days=ee.Number(days)\n",
    "            # get 16-day interval sequence of ndvi data from modis\n",
    "            sequence=nasa_modis_terraveg.filterDate(timestamp.subtract(days.multiply(86400000)),timestamp).select('NDVI')\n",
    "            # calculate and return z score based on mean & stdev of ndvi\n",
    "            return sequence.sort('system:time_start',False).first().subtract(sequence.mean()).divide(sequence.reduce(ee.Reducer.stdDev()).max(1e-6)).resample('bilinear').rename([ee.String('ndvi_anom_').cat(days.format()).cat('d')])\n",
    "        ndvi_anom=_ndvi_anom(timestamp,ndvi_days)\n",
    "\n",
    "        # longitude-latitude trig encoding\n",
    "        def coord_encoding():\n",
    "            # return an encoding for coordinates so the model can be somewhat spatially conscious\n",
    "            lonlat=ee.Image.pixelLonLat()\n",
    "            lat_rad=lonlat.select('latitude').multiply(math.pi/180)\n",
    "            lon_rad=lonlat.select('longitude').multiply(math.pi/180)\n",
    "            return ee.Image.cat(lat_rad.sin().rename('lat_sin'),lat_rad.cos().rename('lat_cos'),lon_rad.sin().rename('lon_sin'),lon_rad.cos().rename('lon_cos'))\n",
    "        lonlat_trig=coord_encoding() # sin and cos of longitude and latitute each for spatial encoding (4 bands)\n",
    "        \n",
    "        # stack all static bands and sample (also added esa_worldcover which doesn't require any processing), return\n",
    "        static_img=ee.Image.cat([\n",
    "            terrain_slope,\n",
    "            aspect_sin,\n",
    "            aspect_cos,\n",
    "            tri,\n",
    "            twi,\n",
    "            curvature,\n",
    "            sand_pc,\n",
    "            clay_pc,\n",
    "            esa_worldcover,\n",
    "            vwc,\n",
    "            ndvi_anom,\n",
    "            lonlat_trig])\n",
    "        return static_img\n",
    "    \n",
    "    # concatenate dynamic & static images\n",
    "    dynamic_arr_img=phi_dynamic()\n",
    "    static_img=phi_static()\n",
    "    img=ee.Image.cat([dynamic_arr_img,static_img])\n",
    "    \n",
    "    # figure out which channels to keep (like the array-based ones in favor of collapsed ones as well as timestamps, u values, and other helper properties)\n",
    "    arr_names=dynamic_arr_img.bandNames().filter(ee.Filter.stringEndsWith('item','_arr'))\n",
    "    scalar_names=arr_names.map(lambda n:ee.String(n).replace('_arr$',''))\n",
    "    dynamic_non_array_names=dynamic_arr_img.bandNames().filter(ee.Filter.stringEndsWith('item','_arr').Not())\n",
    "    to_keep=ee.List(scalar_names.cat(dynamic_non_array_names).cat(static_img.bandNames()))\n",
    "\n",
    "    # sample final image and collapse array properties\n",
    "    samples=img.sampleRegions(collection=subsamples,scale=SCALE_METRIC,tileScale=config_features.get('tileScale'),geometries=False)\n",
    "    collapsed=array_collapse(samples,arr_names)\n",
    "    # compute featurs as a dataframe, return\n",
    "    dF=ee.data.computeFeatures({'expression':collapsed.select(to_keep),'fileFormat':'PANDAS_DATAFRAME'}).drop('geo',errors='ignore',axis=1)\n",
    "    \n",
    "    # new snow flag feature construction, identical to the one in features.ipynb\n",
    "    snow_covers=[col for col in dF.columns if 'snow_cover' in col]\n",
    "    snow_depths=[col for col in dF.columns if 'snow_depth' in col]\n",
    "    snow_flag=(dF[snow_covers].sum(axis=1)>0)|(dF[snow_depths].sum(axis=1)>0)\n",
    "    dF=dF.drop(columns=snow_covers+snow_depths) # drop original snow channels\n",
    "    dF['snow_flag']=snow_flag.astype(int)\n",
    "\n",
    "    # this is also new; moving snow flag to the end of the dataframe to match the PCA ordering done originally, also using ftk before that to perform data normalizations\n",
    "    # also mostly copied from features.ipynb \n",
    "    dF2M=ftk.transform_full(dF,[],ignore,config_full.get('features')['path']['spec'])[0] # transform using ftk\n",
    "    pca_model=joblib.load(config_path.get('pca_persist')) # load in pca model from earlier in this notebook\n",
    "    return pd.DataFrame(pca_model.transform(dF2M.astype('float32',copy=False))).add_prefix('pca_'),subsamples # transform normalized features, return as dataframe (also returning, in this version, the subsamples featurecollection for visualization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b049cd",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "a2b7230e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATLAS: loaded superfold 1 @ database/models/rff_lgcp_sf1.weights.h5\n",
      "[logit] fitted: R=σ(0.9800·μ+0.9339)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 17:08:34.972268: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_14}}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-5 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-5 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-5 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-5 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-5 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-5 div.sk-toggleable__content {\n",
       "  display: none;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  overflow: visible;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-5 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-5 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-5 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-5 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-5 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-5 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-5 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".estimator-table summary {\n",
       "    padding: .5rem;\n",
       "    font-family: monospace;\n",
       "    cursor: pointer;\n",
       "}\n",
       "\n",
       ".estimator-table details[open] {\n",
       "    padding-left: 0.1rem;\n",
       "    padding-right: 0.1rem;\n",
       "    padding-bottom: 0.3rem;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table {\n",
       "    margin-left: auto !important;\n",
       "    margin-right: auto !important;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:nth-child(odd) {\n",
       "    background-color: #fff;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:nth-child(even) {\n",
       "    background-color: #f6f6f6;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:hover {\n",
       "    background-color: #e0e0e0;\n",
       "}\n",
       "\n",
       ".estimator-table table td {\n",
       "    border: 1px solid rgba(106, 105, 104, 0.232);\n",
       "}\n",
       "\n",
       ".user-set td {\n",
       "    color:rgb(255, 94, 0);\n",
       "    text-align: left;\n",
       "}\n",
       "\n",
       ".user-set td.value pre {\n",
       "    color:rgb(255, 94, 0) !important;\n",
       "    background-color: transparent !important;\n",
       "}\n",
       "\n",
       ".default td {\n",
       "    color: black;\n",
       "    text-align: left;\n",
       "}\n",
       "\n",
       ".user-set td i,\n",
       ".default td i {\n",
       "    color: black;\n",
       "}\n",
       "\n",
       ".copy-paste-icon {\n",
       "    background-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCA0NDggNTEyIj48IS0tIUZvbnQgQXdlc29tZSBGcmVlIDYuNy4yIGJ5IEBmb250YXdlc29tZSAtIGh0dHBzOi8vZm9udGF3ZXNvbWUuY29tIExpY2Vuc2UgLSBodHRwczovL2ZvbnRhd2Vzb21lLmNvbS9saWNlbnNlL2ZyZWUgQ29weXJpZ2h0IDIwMjUgRm9udGljb25zLCBJbmMuLS0+PHBhdGggZD0iTTIwOCAwTDMzMi4xIDBjMTIuNyAwIDI0LjkgNS4xIDMzLjkgMTQuMWw2Ny45IDY3LjljOSA5IDE0LjEgMjEuMiAxNC4xIDMzLjlMNDQ4IDMzNmMwIDI2LjUtMjEuNSA0OC00OCA0OGwtMTkyIDBjLTI2LjUgMC00OC0yMS41LTQ4LTQ4bDAtMjg4YzAtMjYuNSAyMS41LTQ4IDQ4LTQ4ek00OCAxMjhsODAgMCAwIDY0LTY0IDAgMCAyNTYgMTkyIDAgMC0zMiA2NCAwIDAgNDhjMCAyNi41LTIxLjUgNDgtNDggNDhMNDggNTEyYy0yNi41IDAtNDgtMjEuNS00OC00OEwwIDE3NmMwLTI2LjUgMjEuNS00OCA0OC00OHoiLz48L3N2Zz4=);\n",
       "    background-repeat: no-repeat;\n",
       "    background-size: 14px 14px;\n",
       "    background-position: 0;\n",
       "    display: inline-block;\n",
       "    width: 14px;\n",
       "    height: 14px;\n",
       "    cursor: pointer;\n",
       "}\n",
       "</style><body><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=500)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LogisticRegression</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.7/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('penalty',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">penalty&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;l2&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('dual',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">dual&nbsp;</td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('tol',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">tol&nbsp;</td>\n",
       "            <td class=\"value\">0.0001</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('C',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">C&nbsp;</td>\n",
       "            <td class=\"value\">1.0</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('fit_intercept',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">fit_intercept&nbsp;</td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('intercept_scaling',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">intercept_scaling&nbsp;</td>\n",
       "            <td class=\"value\">1</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('class_weight',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">class_weight&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('random_state',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">random_state&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('solver',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">solver&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;lbfgs&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('max_iter',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">max_iter&nbsp;</td>\n",
       "            <td class=\"value\">500</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('multi_class',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">multi_class&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;deprecated&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('verbose',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">verbose&nbsp;</td>\n",
       "            <td class=\"value\">0</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('warm_start',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">warm_start&nbsp;</td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('n_jobs',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">n_jobs&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('l1_ratio',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">l1_ratio&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div></div></div><script>function copyToClipboard(text, element) {\n",
       "    // Get the parameter prefix from the closest toggleable content\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${text}` : text;\n",
       "\n",
       "    const originalStyle = element.style;\n",
       "    const computedStyle = window.getComputedStyle(element);\n",
       "    const originalWidth = computedStyle.width;\n",
       "    const originalHTML = element.innerHTML.replace('Copied!', '');\n",
       "\n",
       "    navigator.clipboard.writeText(fullParamName)\n",
       "        .then(() => {\n",
       "            element.style.width = originalWidth;\n",
       "            element.style.color = 'green';\n",
       "            element.innerHTML = \"Copied!\";\n",
       "\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        })\n",
       "        .catch(err => {\n",
       "            console.error('Failed to copy:', err);\n",
       "            element.style.color = 'red';\n",
       "            element.innerHTML = \"Failed!\";\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        });\n",
       "    return false;\n",
       "}\n",
       "\n",
       "document.querySelectorAll('.fa-regular.fa-copy').forEach(function(element) {\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const paramName = element.parentElement.nextElementSibling.textContent.trim();\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${paramName}` : paramName;\n",
       "\n",
       "    element.setAttribute('title', fullParamName);\n",
       "});\n",
       "</script></body>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=500)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize ATLAS\n",
    "atlasv2=ATLAS(dF_pca,merged_splits,H,sf_id=1)\n",
    "\n",
    "# fit once and save calibrator then plot evaluation chart:\n",
    "atlasv2.fit_logit_calibrator(save_path=os.path.join(config_path.get('models'), f'logit_sf{atlasv2.sf_id}.joblib'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87705f6a",
   "metadata": {},
   "source": [
    "### Example Usage\n",
    "see: \n",
    "https://en.wikipedia.org/wiki/2020_Pettimudi_landslide & \n",
    "https://www.thehindu.com/specials/text-and-context/landslides-in-pettimudi-social-inequalities-in-disasters/article65939327.ece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "49b3490d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "point=(10.01776704920028,76.97837079702704) # pettimudi, idukki, india\n",
    "date='6 August 2020' # the model should find a 47% chance of landslide within the next hour which would have exceeded the action threshold and triggered a warning beforehand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "c76ba922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 18:11:28.632743: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_14}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.47112528681848864"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output0=phi(pointer=ee.Geometry.Point(point[1],point[0]),timestamp=ee.Number(int(datetime.strptime(date,'%d %B %Y %H:%M' if ':' in date else '%d %B %Y').replace(tzinfo=timezone.utc).timestamp()*1000)),radius=ee.Number(17),K=64)\n",
    "testdF=output0[0] # only getting risk score\n",
    "atlasv2.risk_score(testdF,17**2*math.pi,3600)[1] # feature dF (from phi), window area (17km radius), window interval (1h, in seconds), select only 1st element (risk score, since 0th element returns lambda absolute intensity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "be32c698",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: database/outputs/examples/pettimudi_6-9-20\n"
     ]
    }
   ],
   "source": [
    "# saving example for later for display & vis\n",
    "savename='pettimudi_6-9-20'\n",
    "dir_path=os.path.join('database/outputs/examples',savename)\n",
    "os.makedirs(dir_path,exist_ok=True)\n",
    "\n",
    "# saving both risk assessment score and points featurecollection from inference step\n",
    "dFoutput,fc=output0 # phi output\n",
    "risk=atlasv2.risk_score(dFoutput,17**2*math.pi,3600)[1]\n",
    "with open(os.path.join(dir_path,'tuple.pkl'),'wb') as f:pickle.dump(risk,f,pickle.HIGHEST_PROTOCOL)\n",
    "with open(os.path.join(dir_path,'featurecollection.json'),'w') as f:json.dump(fc.getInfo(),f)\n",
    "\n",
    "print(f'saved to: {dir_path}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
